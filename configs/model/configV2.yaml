class_path: cmmvae.models.CMMVAEModel
init_args:
  kl_annealing_fn:
    class_path: cmmvae.modules.base.KLAnnealingFn
    init_args:
      kl_weight: 1.0e-07
  record_gradients: false
  adv_weight: 1
  gradient_record_cap: 20
  module:
    class_path: cmmvae.modules.CMMVAE
    init_args:
      vae:
        class_path: cmmvae.modules.CLVAE
        init_args:
          latent_dim: 256
          encoder_config:
            class_path: cmmvae.modules.base.FCBlockConfig
            init_args:
              layers: [ 768, 512 ]
              dropout_rate: 0.1
              use_batch_norm: True
              use_layer_norm: False
              activation_fn: torch.nn.ReLU
              return_hidden: False
          decoder_config:
            class_path: cmmvae.modules.base.FCBlockConfig
            init_args:
              layers: [ 256, 512, 768 ]
              dropout_rate: 0.0
              use_batch_norm: False
              use_layer_norm: False
              activation_fn: torch.nn.ReLU
          conditional_config:
            class_path: cmmvae.modules.base.FCBlockConfig
            init_args:
              layers:
              - 256
              dropout_rate: 0.0
              use_batch_norm: False
              use_layer_norm: True
              activation_fn: null
      experts:
        class_path: cmmvae.modules.base.Experts
        init_args:
          experts:
          - class_path: cmmvae.modules.base.Expert
            init_args:
              id: human
              encoder_config:
                class_path: cmmvae.modules.base.FCBlockConfig
                init_args:
                  layers: [ 60664, 1024, 768 ]
                  dropout_rate: [ 0.1, 0.1 ]
                  use_batch_norm: True
                  use_layer_norm: False
                  activation_fn: torch.nn.ReLU
              decoder_config:
                class_path: cmmvae.modules.base.FCBlockConfig
                init_args:
                  layers: [ 768, 1024, 60664 ]
                  dropout_rate: 0.0
                  use_batch_norm: False
                  use_layer_norm: False
                  activation_fn: torch.nn.ReLU
          - class_path: cmmvae.modules.base.Expert
            init_args:
              id: mouse
              encoder_config:
                class_path: cmmvae.modules.base.FCBlockConfig
                init_args:
                  layers: [ 52417, 1024, 768 ]
                  dropout_rate: [ 0.1, 0.1 ]
                  use_batch_norm: True
                  use_layer_norm: False
                  activation_fn: torch.nn.ReLU
              decoder_config:
                class_path: cmmvae.modules.base.FCBlockConfig
                init_args:
                  layers: [ 768, 1024, 52417 ]
                  dropout_rate: 0.0
                  use_batch_norm: False
                  use_layer_norm: False
                  activation_fn: torch.nn.ReLU
      adversarials:
      # - class_path: cmmvae.modules.base.FCBlockConfig
      #   init_args:
      #     layers: [ 256, 32, 32, 1 ]
      #     dropout_rate: 0.0
      #     use_batch_norm: False
      #     use_layer_norm: False
      #     activation_fn: torch.nn.ReLU
      # - class_path: cmmvae.modules.base.FCBlockConfig
      #   init_args:
      #     layers: [ 256, 32, 32, 1 ]
      #     dropout_rate: 0.0
      #     use_batch_norm: False
      #     use_layer_norm: False
      #     activation_fn: torch.nn.ReLU
      # - class_path: cmmvae.modules.base.FCBlockConfig
      #   init_args:
      #     layers: [ 128, 32, 32, 1 ]
      #     dropout_rate: 0.0
      #     use_batch_norm: False
      #     use_layer_norm: False
      #     activation_fn: torch.nn.ReLU
