class_path: cmmvae.models.CMMVAEModel
init_args:
  kl_annealing_fn:
    class_path: cmmvae.modules.base.annealing_fn.LinearKLAnnealingFn
    init_args:
      min_kl_weight: 0.1
      max_kl_weight: 1.0
      warmup_steps: 1e4
      climax_steps: 4e4
  record_gradients: false
  adv_weight: 1
  gradient_record_cap: 20
  module:
    class_path: cmmvae.modules.CMMVAE
    init_args:
      vae:
        class_path: cmmvae.modules.CLVAE
        init_args:
          latent_dim: 128
          conditionals:
          - assay
          - donor_id
          - dataset_id
          - species
          encoder_config:
            class_path: cmmvae.modules.base.FCBlockConfig
            init_args:
              layers: [ 768, 512, 256, 256 ]
              dropout_rate: 0.0
              use_batch_norm: False
              use_layer_norm: True
              activation_fn: torch.nn.ReLU
              return_hidden:
              - false
              - true
              - true
          decoder_config:
            class_path: cmmvae.modules.base.FCBlockConfig
            init_args:
              layers: [ 128, 256, 256, 512, 768 ]
              dropout_rate: 0.0
              use_batch_norm: False
              use_layer_norm: False
              activation_fn: torch.nn.ReLU
          conditional_config:
            class_path: cmmvae.modules.base.FCBlockConfig
            init_args:
              layers:
              - 128
              dropout_rate: 0.0
              use_batch_norm: False
              use_layer_norm: True
              activation_fn: null
      experts:
        class_path: cmmvae.modules.base.Experts
        init_args:
          experts:
          - class_path: cmmvae.modules.base.Expert
            init_args:
              id: human
              encoder_config:
                class_path: cmmvae.modules.base.FCBlockConfig
                init_args:
                  layers: [ 60530, 1024, 512 ]
                  dropout_rate: [ 0.1, 0.1 ]
                  use_batch_norm: True
                  use_layer_norm: False
                  activation_fn: torch.nn.ReLU
              decoder_config:
                class_path: cmmvae.modules.base.FCBlockConfig
                init_args:
                  layers: [ 512, 1024, 60530 ]
                  dropout_rate: 0.0
                  use_batch_norm: False
                  use_layer_norm: False
                  activation_fn: torch.nn.ReLU
          - class_path: cmmvae.modules.base.Expert
            init_args:
              id: mouse
              encoder_config:
                class_path: cmmvae.modules.base.FCBlockConfig
                init_args:
                  layers: [ 52437, 1024, 512 ]
                  dropout_rate: [ 0.1, 0.1 ]
                  use_batch_norm: True
                  use_layer_norm: False
                  activation_fn: torch.nn.ReLU
              decoder_config:
                class_path: cmmvae.modules.base.FCBlockConfig
                init_args:
                  layers: [ 512, 1024, 52437 ]
                  dropout_rate: 0.0
                  use_batch_norm: False
                  use_layer_norm: False
                  activation_fn: torch.nn.ReLU
      adversarials:
      # - class_path: cmmvae.modules.base.FCBlockConfig
      #   init_args:
      #     layers: [ 256, 32, 32, 1 ]
      #     dropout_rate: 0.0
      #     use_batch_norm: False
      #     use_layer_norm: True
      #     activation_fn:
      #     - torch.nn.ReLU
      #     - torch.nn.ReLU
      #     - null
      # - class_path: cmmvae.modules.base.FCBlockConfig
      #   init_args:
      #     layers: [ 256, 32, 32, 1 ]
      #     dropout_rate: 0.0
      #     use_batch_norm: False
      #     use_layer_norm: True
      #     activation_fn:
      #     - torch.nn.ReLU
      #     - torch.nn.ReLU
      #     - null
      # - class_path: cmmvae.modules.base.FCBlockConfig
      #   init_args:
      #     layers: [ 128, 32, 32, 1 ]
      #     dropout_rate: 0.0
      #     use_batch_norm: False
      #     use_layer_norm: True
      #     activation_fn:
      #     - torch.nn.ReLU
      #     - torch.nn.ReLU
      #     - null
